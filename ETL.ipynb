{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Sistema Bancario - Projeto Integrador Grupo 05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurando o ambiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/azureuser/.local/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /home/azureuser/.local/lib/python3.8/site-packages (from pyspark) (0.10.9.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
    "!tar -xvzf spark-3.3.2-bin-hadoop3.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/azureuser/spark-3.3.2-bin-hadoop3\"\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"Read CSV\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando os dados:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+-------------------+----------------+\n",
      "| id|                nome|               email|      data_cadastro|        telefone|\n",
      "+---+--------------------+--------------------+-------------------+----------------+\n",
      "|641|Priscila Felix do...|priscila-felix-do...|2021-03-28 18:46:57|+55(30)2227-2428|\n",
      "| 94|             idelmon|idelmon_94@gmail.com|2019-09-19 12:33:19|+55(29)3027-2026|\n",
      "|584|Liliane soares da...|liliane-soares-da...|2021-02-10 19:15:30|+55(21)2024-2520|\n",
      "|580|Fagner jose dos s...|fagner-jose-dos-s...|2021-02-07 01:47:04|+55(24)2624-2029|\n",
      "| 21|               Cildo|  cildo_21@gmail.com|2019-07-30 11:40:10|+55(21)2222-2422|\n",
      "|582|Nielton da Silva ...|nielton-da-silva-...|2021-02-09 00:11:22|+55(27)2028-2828|\n",
      "|586|Armando Teles da ...|armando-teles-da-...|2021-02-12 15:20:14|+55(27)2720-2230|\n",
      "|151|            Fabricio|fabricio_151@gmai...|2019-10-14 21:16:27|+55(20)2121-2326|\n",
      "| 83|       Flavio junior|flavio-junior_83@...|2019-09-11 15:24:00|+55(22)2028-2122|\n",
      "|587|              Wagner|wagner_587@gmail.com|2021-02-14 13:56:25|+55(28)2620-2421|\n",
      "|585|         Jeine maria|jeine-maria_585@g...|2021-02-11 23:49:04|+55(27)2528-2523|\n",
      "|534|                Thay|  thay_534@gmail.com|2020-12-19 02:10:58|+55(29)2327-2626|\n",
      "|639|Sindini vieira do...|sindini-vieira-do...|2021-03-21 16:07:04|+55(28)2126-2123|\n",
      "|528|           Aparecido|aparecido_528@gma...|2020-12-12 20:02:38|+55(26)2721-2828|\n",
      "|636|         Edy Navarro|edy-navarro_636@g...|2021-03-04 11:29:03|+55(30)3024-2821|\n",
      "|626|        Moab Tenorio|moab-tenorio_626@...|2021-02-21 14:49:26|+55(29)2921-2730|\n",
      "|627|     Jose Franciusci|jose-franciusci_6...|2021-02-21 14:53:39|+55(29)2222-2227|\n",
      "|637|Crislane luiz da ...|crislane-luiz-da-...|2021-03-07 03:11:09|+55(30)2821-2024|\n",
      "|635|             Emerson|emerson_635@gmail...|2021-03-03 23:03:31|+55(20)2224-2829|\n",
      "|638|               Italo| italo_638@gmail.com|2021-03-15 23:19:48|+55(25)2323-2424|\n",
      "+---+--------------------+--------------------+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# leitura de todos os arquivos CSV na pasta clientes\n",
    "clientes = spark.read.option(\"sep\", \";\").schema(\"id int, nome string, email string, data_cadastro timestamp, telefone string\").option(\"header\", \"false\").csv(\"dados/clientes/*.csv\")\n",
    "\n",
    "# filtrando o dataframe para excluir o arquivo clientes-001.csv\n",
    "clientes_sem_header = clientes.filter(~input_file_name().rlike(\"clients-001.csv\"))\n",
    "clientes_com_header = spark.read.option(\"sep\", \";\").schema(\"id int, nome string, email string, data_cadastro timestamp, telefone string\").option(\"header\", \"true\").csv(\"dados/clientes/clients-001.csv\")\n",
    "\n",
    "# unindo os dois dataframes\n",
    "clientes = clientes_com_header.union(clientes_sem_header)\n",
    "\n",
    "clientes.count()\n",
    "clientes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction-in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----+-------------------+\n",
      "|  id|cliente_id|valor|               data|\n",
      "+----+----------+-----+-------------------+\n",
      "|8615|       586|  0.2|2022-01-19 20:12:26|\n",
      "|8613|       586|  0.2|2022-01-19 20:11:25|\n",
      "|8611|       586|  0.2|2022-01-19 20:10:05|\n",
      "|8606|       910|300.0|2022-01-19 19:59:36|\n",
      "|8604|        76|100.0|2022-01-18 12:48:14|\n",
      "|8603|        76|100.0|2022-01-18 12:48:04|\n",
      "|8602|        76|100.0|2022-01-18 12:47:47|\n",
      "|8601|        76|100.0|2022-01-18 12:47:43|\n",
      "|8600|        76|100.0|2022-01-18 12:47:39|\n",
      "|8599|        76|100.0|2022-01-18 12:43:05|\n",
      "|8598|        76|100.0|2022-01-18 12:42:56|\n",
      "|8597|        76|100.0|2022-01-18 12:40:28|\n",
      "|8596|        76|100.0|2022-01-18 12:38:19|\n",
      "|8595|        76|100.0|2022-01-18 12:37:59|\n",
      "|8594|        76|100.0|2022-01-18 12:37:29|\n",
      "|8593|        76|100.0|2022-01-18 12:37:19|\n",
      "|8592|       907| 10.0|2022-01-18 12:30:26|\n",
      "|8591|       907| 10.0|2022-01-18 12:30:14|\n",
      "|8590|       907| 10.0|2022-01-18 12:30:10|\n",
      "|8589|       907| 10.0|2022-01-18 12:30:04|\n",
      "+----+----------+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# leitura de todos os arquivos CSV na pasta transaction-in\n",
    "transaction_in = spark.read.option(\"sep\", \";\").schema(\"id int, cliente_id int, valor double, data timestamp\").option(\"header\", \"false\").csv(\"/home/azureuser/transaction-in/*.csv\")\n",
    "\n",
    "# filtrando o dataframe para excluir o arquivo transaction-in-001.csv\n",
    "transaction_in_sem_header = transaction_in.filter(~input_file_name().rlike(\"transaction-in-001.csv\"))\n",
    "transaction_in_com_header = spark.read.option(\"sep\", \";\").schema(\"id int, cliente_id int, valor double, data timestamp\").option(\"header\", \"true\").csv(\"/home/azureuser/transaction-in/transaction-in-001.csv\")\n",
    "\n",
    "# unindo os dois dataframes\n",
    "transaction_in = transaction_in_com_header.union(transaction_in_sem_header)\n",
    "\n",
    "transaction_in.count()\n",
    "transaction_in.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction-out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+-------------------+\n",
      "|  id|cliente_id| valor|               data|\n",
      "+----+----------+------+-------------------+\n",
      "|8607|       910|  -2.0|2022-01-19 20:15:26|\n",
      "|8608|       910|  -2.0|2022-01-19 20:14:56|\n",
      "|8609|       910|  -2.0|2022-01-19 20:14:26|\n",
      "|8610|       910|  -2.0|2022-01-19 20:13:56|\n",
      "|8612|       910|  -2.0|2022-01-19 20:13:26|\n",
      "|8614|       910|  -2.0|2022-01-19 20:12:56|\n",
      "|8573|       671| -10.0|2022-01-13 15:21:25|\n",
      "|8574|       671| -10.0|2022-01-13 15:20:55|\n",
      "|8575|       671|  -5.0|2022-01-13 15:20:25|\n",
      "|8576|       671| -10.0|2022-01-13 15:19:55|\n",
      "|8577|       671|  -2.0|2022-01-13 15:19:25|\n",
      "|8580|       671| -10.0|2022-01-13 15:18:55|\n",
      "|8581|       671|-100.0|2022-01-13 15:18:25|\n",
      "|8582|       671| -15.0|2022-01-13 15:17:55|\n",
      "|8583|       671|-100.0|2022-01-13 15:17:25|\n",
      "|8584|       671|  -2.0|2022-01-13 15:16:55|\n",
      "|8579|       370|  -7.0|2022-01-13 03:44:57|\n",
      "|8578|       370|  -3.0|2022-01-13 03:37:43|\n",
      "|8572|       671|  -2.0|2022-01-12 19:43:20|\n",
      "|8539|       671| -10.0|2022-01-12 17:16:29|\n",
      "+----+----------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transaction_out = spark.read.option(\"sep\", \";\").schema(\"id int, cliente_id int, valor double, data timestamp\").option(\"header\", \"false\").csv(\"/home/azureuser/transaction-out/*.csv\")\n",
    "\n",
    "transaction_out_sem_header = transaction_out.filter(~input_file_name().rlike(\"transaction-out-001.csv\"))\n",
    "transaction_out_com_header = spark.read.option(\"sep\", \";\")\\\n",
    "    .schema(\"id int, cliente_id int, valor double, data timestamp\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(\"/home/azureuser/transaction-out/transaction-out-001.csv\")\n",
    "\n",
    "transaction_out = transaction_out_com_header.union(transaction_out_sem_header)\n",
    "\n",
    "transaction_out.count()\n",
    "transaction_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análises/Transformações para identificar as fraudes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unindo as duas tabelas de transações (in, out):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem de todas as transações: 7272\n",
      "+----+----------+-----+-------------------+\n",
      "|id  |cliente_id|valor|data               |\n",
      "+----+----------+-----+-------------------+\n",
      "|8615|586       |0.2  |2022-01-19 20:12:26|\n",
      "|8613|586       |0.2  |2022-01-19 20:11:25|\n",
      "|8611|586       |0.2  |2022-01-19 20:10:05|\n",
      "|8606|910       |300.0|2022-01-19 19:59:36|\n",
      "|8604|76        |100.0|2022-01-18 12:48:14|\n",
      "|8603|76        |100.0|2022-01-18 12:48:04|\n",
      "|8602|76        |100.0|2022-01-18 12:47:47|\n",
      "|8601|76        |100.0|2022-01-18 12:47:43|\n",
      "|8600|76        |100.0|2022-01-18 12:47:39|\n",
      "|8599|76        |100.0|2022-01-18 12:43:05|\n",
      "|8598|76        |100.0|2022-01-18 12:42:56|\n",
      "|8597|76        |100.0|2022-01-18 12:40:28|\n",
      "|8596|76        |100.0|2022-01-18 12:38:19|\n",
      "|8595|76        |100.0|2022-01-18 12:37:59|\n",
      "|8594|76        |100.0|2022-01-18 12:37:29|\n",
      "|8593|76        |100.0|2022-01-18 12:37:19|\n",
      "|8592|907       |10.0 |2022-01-18 12:30:26|\n",
      "|8591|907       |10.0 |2022-01-18 12:30:14|\n",
      "|8590|907       |10.0 |2022-01-18 12:30:10|\n",
      "|8589|907       |10.0 |2022-01-18 12:30:04|\n",
      "+----+----------+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_transactions ----- O Dataframe de todas as transações (in e out)\n",
    "df_transactions = (transaction_in.union(transaction_out))\n",
    "\n",
    "print(f\"Contagem de todas as transações: {df_transactions.count()}\")\n",
    "\n",
    "df_transactions.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adicionando na tabela df_transaction uma coluna para a diferença de tempo entre as transações:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-------------------+------------------+\n",
      "| id|cliente_id|valor|               data|diferenca_de_tempo|\n",
      "+---+----------+-----+-------------------+------------------+\n",
      "| 58|         2|-20.0|2019-09-23 22:49:44|                 0|\n",
      "| 60|         2| -5.0|2019-09-28 02:12:19|            357755|\n",
      "| 99|         2| -2.0|2019-10-02 18:21:43|            403764|\n",
      "|100|         2| -2.0|2019-10-02 18:23:58|               135|\n",
      "|101|         2|-10.0|2019-10-02 19:37:18|              4400|\n",
      "|220|         2| -2.0|2019-11-27 22:38:20|           4849262|\n",
      "|226|         2| -2.0|2019-11-28 19:58:05|             76785|\n",
      "|227|         2| -2.0|2019-11-28 19:58:24|                19|\n",
      "|228|         2| -2.0|2019-11-28 19:58:44|                20|\n",
      "|231|         2| -2.0|2019-11-30 14:38:14|            153570|\n",
      "|232|         2| -2.0|2019-11-30 14:38:49|                35|\n",
      "|233|         2| -2.0|2019-11-30 14:41:20|               151|\n",
      "|234|         2| -2.0|2019-11-30 14:45:02|               222|\n",
      "|244|         2| -2.0|2019-12-03 19:48:31|            277409|\n",
      "|245|         2| -2.0|2019-12-03 19:48:51|                20|\n",
      "|246|         2| -2.0|2019-12-03 19:49:08|                17|\n",
      "|247|         2| -2.0|2019-12-03 19:49:40|                32|\n",
      "|249|         2| -2.0|2019-12-04 01:19:58|             19818|\n",
      "|250|         2| -2.0|2019-12-04 01:21:54|               116|\n",
      "|251|         2| -2.0|2019-12-04 01:23:02|                68|\n",
      "+---+----------+-----+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag, datediff, col, when, lit,desc, asc\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Essa window irá particionar a tabela através do cliente_id e ordenar essa partição por data\n",
    "window_spec = Window.partitionBy(\"cliente_id\").orderBy(\"data\")\n",
    "\n",
    "# Essa parte cria a coluna \"diferenca_de_tempo\" para o tempo entre as transações (obs.: O tempo será 0, quando a diferenca de tempo for nula, ou seja, quando subsequente houver um outro cliente e não houver diferença de tempo entre eles.\n",
    "diferenca_de_tempo = (col(\"data\").cast(\"long\") - \n",
    "                   lag(col(\"data\")).over(window_spec).cast(\"long\"))\n",
    "\n",
    "df_transactions = df_transactions.withColumn(\"diferenca_de_tempo\", when(diferenca_de_tempo.isNull(), lit(0)).otherwise(diferenca_de_tempo))\n",
    "\n",
    "df_transactions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adicionando a coluna 'fraudado', do tipo boolean, à tabela df_transactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-------------------+------------------+--------+\n",
      "| id|cliente_id|valor|               data|diferenca_de_tempo|fraudado|\n",
      "+---+----------+-----+-------------------+------------------+--------+\n",
      "| 58|         2|-20.0|2019-09-23 22:49:44|                 0|   false|\n",
      "| 60|         2| -5.0|2019-09-28 02:12:19|            357755|   false|\n",
      "| 99|         2| -2.0|2019-10-02 18:21:43|            403764|   false|\n",
      "|100|         2| -2.0|2019-10-02 18:23:58|               135|   false|\n",
      "|101|         2|-10.0|2019-10-02 19:37:18|              4400|   false|\n",
      "|220|         2| -2.0|2019-11-27 22:38:20|           4849262|   false|\n",
      "|226|         2| -2.0|2019-11-28 19:58:05|             76785|   false|\n",
      "|227|         2| -2.0|2019-11-28 19:58:24|                19|    true|\n",
      "|228|         2| -2.0|2019-11-28 19:58:44|                20|    true|\n",
      "|231|         2| -2.0|2019-11-30 14:38:14|            153570|   false|\n",
      "|232|         2| -2.0|2019-11-30 14:38:49|                35|    true|\n",
      "|233|         2| -2.0|2019-11-30 14:41:20|               151|   false|\n",
      "|234|         2| -2.0|2019-11-30 14:45:02|               222|   false|\n",
      "|244|         2| -2.0|2019-12-03 19:48:31|            277409|   false|\n",
      "|245|         2| -2.0|2019-12-03 19:48:51|                20|    true|\n",
      "|246|         2| -2.0|2019-12-03 19:49:08|                17|    true|\n",
      "|247|         2| -2.0|2019-12-03 19:49:40|                32|    true|\n",
      "|249|         2| -2.0|2019-12-04 01:19:58|             19818|   false|\n",
      "|250|         2| -2.0|2019-12-04 01:21:54|               116|    true|\n",
      "|251|         2| -2.0|2019-12-04 01:23:02|                68|    true|\n",
      "+---+----------+-----+-------------------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "df_transactions = df_transactions\\\n",
    "    .withColumn(\"fraudado\", when((col(\"diferenca_de_tempo\") <= 120) & (col(\"diferenca_de_tempo\") != 0), True)\\\n",
    "    .otherwise(False))\n",
    "\n",
    "df_transactions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisando o DataFrame gerado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions.createOrReplaceTempView(\"transacoes_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|transacoes_fraudadas|\n",
      "+--------------------+\n",
      "|                2008|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT count(id) AS transacoes_fraudadas \n",
    "    FROM transacoes_view \n",
    "    WHERE fraudado == True\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+\n",
      "|cliente_id|transacoes_por_cliente|\n",
      "+----------+----------------------+\n",
      "|       211|                   113|\n",
      "|        76|                    12|\n",
      "|       671|                  1069|\n",
      "|       431|                    93|\n",
      "|       732|                   933|\n",
      "|       586|                    85|\n",
      "|       907|                     5|\n",
      "|       730|                   685|\n",
      "|       702|                   191|\n",
      "|       370|                   346|\n",
      "|       910|                     7|\n",
      "|       909|                     1|\n",
      "|        74|                   582|\n",
      "|       737|                   150|\n",
      "|       530|                     6|\n",
      "|       731|                    21|\n",
      "|        44|                    25|\n",
      "|       663|                     1|\n",
      "|       685|                    11|\n",
      "|       727|                     5|\n",
      "+----------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT cliente_id, COUNT(cliente_id) AS transacoes_por_cliente \n",
    "    FROM transacoes_view \n",
    "    GROUP BY cliente_id\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|cliente_id|qtd_fraudes_por_cliente|\n",
      "+----------+-----------------------+\n",
      "|         2|                     22|\n",
      "|         4|                     91|\n",
      "|        44|                      9|\n",
      "|        73|                    227|\n",
      "|        74|                    196|\n",
      "|        76|                      8|\n",
      "|       205|                    170|\n",
      "|       211|                     11|\n",
      "|       335|                     13|\n",
      "|       370|                      1|\n",
      "|       389|                      1|\n",
      "|       431|                      5|\n",
      "|       449|                      4|\n",
      "|       495|                      2|\n",
      "|       515|                     14|\n",
      "|       523|                     91|\n",
      "|       529|                      6|\n",
      "|       530|                      1|\n",
      "|       533|                    132|\n",
      "|       534|                      9|\n",
      "+----------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT cliente_id, COUNT(cliente_id) AS qtd_fraudes_por_cliente \n",
    "    FROM transacoes_view \n",
    "    WHERE fraudado == True \n",
    "    GROUP BY cliente_id\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|total_clientes_distintos|\n",
      "+------------------------+\n",
      "|                      67|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"SELECT COUNT(DISTINCT cliente_id) AS total_clientes_distintos \n",
    "    FROM transacoes_view\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|total_clientes_distintos_com_cartoes_fraudados|\n",
      "+----------------------------------------------+\n",
      "|                                            38|\n",
      "+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"SELECT COUNT(DISTINCT cliente_id) AS total_clientes_distintos_com_cartoes_fraudados \n",
    "    FROM transacoes_view \n",
    "    WHERE fraudado = True\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|quant_clientes_sem_fraude|\n",
      "+-------------------------+\n",
      "|                       29|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quantidade_clientes_sem_fraude = spark.sql(\"SELECT COUNT(*) as quant_clientes_sem_fraude \\\n",
    "                                            FROM ( \\\n",
    "                                            SELECT cliente_id, SUM(CASE WHEN fraudado = True THEN 1 ELSE 0 END) as qtd_fraude \\\n",
    "                                            FROM transacoes_view \\\n",
    "                                            GROUP BY cliente_id \\\n",
    "                                            HAVING qtd_fraude = 0 \\\n",
    "                                            )\")\n",
    "\n",
    "quantidade_clientes_sem_fraude.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes.createOrReplaceTempView(\"clientes_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|total_clientes_distintos|\n",
      "+------------------------+\n",
      "|                      60|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"SELECT COUNT(DISTINCT cliente_id) AS total_clientes_distintos \n",
    "    FROM transacoes_view INNER JOIN clientes_view ON clientes_view.id = transacoes_view.cliente_id\"\"\"\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Migração dos dados do Spark para o banco SQL Server (tabela clientes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Criar uma sessão do Spark\n",
    "spark = SparkSession.builder.appName(\"Exemplo\").getOrCreate()\n",
    "\n",
    "# Criando uma conexão com o SQL Server\n",
    "\n",
    "conn = pyodbc.connect(\"Driver={ODBC Driver 18 for SQL Server};Server=tcp:projeto-integrador5.database.windows.net,1433;Database=projeto_integrador;Uid=ProjetoIntegrador;Pwd=#r00t123;Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "# Cria a tabela \"clientes\" caso ela não exista ou exclui a tabela existente e cria uma nova\n",
    "cursor.execute(\"IF OBJECT_ID('clientes', 'U') IS NOT NULL \"\n",
    "               \"DROP TABLE clientes; \"\n",
    "               \"CREATE TABLE clientes \"\n",
    "               \"( \"\n",
    "               \"id INT PRIMARY KEY, \"\n",
    "               \"nome VARCHAR(100), \"\n",
    "               \"email VARCHAR(100), \"\n",
    "               \"data_cadastro DATETIME, \"\n",
    "               \"telefone VARCHAR(100)\"\n",
    "               \"); \")\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "array_clientes = [row.asDict() for row in clientes.collect()]\n",
    "\n",
    "    \n",
    "for cliente in array_clientes:\n",
    "    cursor.execute(\"INSERT INTO clientes (id, nome, email, data_cadastro, telefone) \"\n",
    "            \"VALUES (?, ?, ?, ?, ?)\",\n",
    "            cliente[\"id\"], cliente[\"nome\"], cliente[\"email\"], cliente[\"data_cadastro\"], cliente[\"telefone\"])\n",
    "    conn.commit()\n",
    "\n",
    "cursor.close()\n",
    "conn.close()  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Migração dos dados do Spark para o banco SQL Server (tabela transações)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Criar uma sessão do Spark\n",
    "spark = SparkSession.builder.appName(\"Exemplo\").getOrCreate()\n",
    "\n",
    "# Criando uma conexão com o SQL Server\n",
    "\n",
    "conn = pyodbc.connect(\"Driver={ODBC Driver 18 for SQL Server};Server=tcp:projeto-integrador5.database.windows.net,1433;Database=projeto_integrador;Uid=ProjetoIntegrador;Pwd=#r00t123;Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "# Cria a tabela \"transacoes\" caso ela não exista ou exclui a tabela existente e cria uma nova\n",
    "cursor.execute(\"IF OBJECT_ID('transacoes', 'U') IS NOT NULL \"\n",
    "               \"DROP TABLE transacoes; \"\n",
    "               \"CREATE TABLE transacoes \"\n",
    "               \"( \"\n",
    "               \"id INT PRIMARY KEY, \"\n",
    "               \"cliente_id INT, \"\n",
    "               \"valor FLOAT, \"\n",
    "               \"data DATETIME, \"\n",
    "               \"diferenca_de_tempo INT, \"\n",
    "               \"fraudado BIT \"\n",
    "               \"); \")\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "array_transaction = [row.asDict() for row in df_transactions.collect()]\n",
    "\n",
    "    \n",
    "for transacao in array_transaction:\n",
    "    cursor.execute(\"INSERT INTO transacoes (id, cliente_id, valor, data, diferenca_de_tempo, fraudado) \"\n",
    "            \"VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "            transacao[\"id\"], transacao[\"cliente_id\"], transacao[\"valor\"], transacao[\"data\"], transacao[\"diferenca_de_tempo\"], transacao[\"fraudado\"])\n",
    "    conn.commit()\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
