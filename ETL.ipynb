{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Sistema Bancario - Projeto Integrador Grupo 05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurando o ambiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/azureuser/.local/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /home/azureuser/.local/lib/python3.8/site-packages (from pyspark) (0.10.9.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
    "!tar -xvzf spark-3.3.2-bin-hadoop3.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/azureuser/spark-3.3.2-bin-hadoop3\"\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"Read CSV\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando os dados:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+-------------------+----------------+\n",
      "| id|                nome|               email|      data_cadastro|        telefone|\n",
      "+---+--------------------+--------------------+-------------------+----------------+\n",
      "|641|Priscila Felix do...|priscila-felix-do...|2021-03-28 18:46:57|+55(30)2227-2428|\n",
      "| 94|             idelmon|idelmon_94@gmail.com|2019-09-19 12:33:19|+55(29)3027-2026|\n",
      "|584|Liliane soares da...|liliane-soares-da...|2021-02-10 19:15:30|+55(21)2024-2520|\n",
      "|580|Fagner jose dos s...|fagner-jose-dos-s...|2021-02-07 01:47:04|+55(24)2624-2029|\n",
      "| 21|               Cildo|  cildo_21@gmail.com|2019-07-30 11:40:10|+55(21)2222-2422|\n",
      "|582|Nielton da Silva ...|nielton-da-silva-...|2021-02-09 00:11:22|+55(27)2028-2828|\n",
      "|586|Armando Teles da ...|armando-teles-da-...|2021-02-12 15:20:14|+55(27)2720-2230|\n",
      "|151|            Fabricio|fabricio_151@gmai...|2019-10-14 21:16:27|+55(20)2121-2326|\n",
      "| 83|       Flavio junior|flavio-junior_83@...|2019-09-11 15:24:00|+55(22)2028-2122|\n",
      "|587|              Wagner|wagner_587@gmail.com|2021-02-14 13:56:25|+55(28)2620-2421|\n",
      "|585|         Jeine maria|jeine-maria_585@g...|2021-02-11 23:49:04|+55(27)2528-2523|\n",
      "|534|                Thay|  thay_534@gmail.com|2020-12-19 02:10:58|+55(29)2327-2626|\n",
      "|639|Sindini vieira do...|sindini-vieira-do...|2021-03-21 16:07:04|+55(28)2126-2123|\n",
      "|528|           Aparecido|aparecido_528@gma...|2020-12-12 20:02:38|+55(26)2721-2828|\n",
      "|636|         Edy Navarro|edy-navarro_636@g...|2021-03-04 11:29:03|+55(30)3024-2821|\n",
      "|626|        Moab Tenorio|moab-tenorio_626@...|2021-02-21 14:49:26|+55(29)2921-2730|\n",
      "|627|     Jose Franciusci|jose-franciusci_6...|2021-02-21 14:53:39|+55(29)2222-2227|\n",
      "|637|Crislane luiz da ...|crislane-luiz-da-...|2021-03-07 03:11:09|+55(30)2821-2024|\n",
      "|635|             Emerson|emerson_635@gmail...|2021-03-03 23:03:31|+55(20)2224-2829|\n",
      "|638|               Italo| italo_638@gmail.com|2021-03-15 23:19:48|+55(25)2323-2424|\n",
      "+---+--------------------+--------------------+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# leitura de todos os arquivos CSV na pasta clientes\n",
    "clientes = spark.read.option(\"sep\", \";\").schema(\"id int, nome string, email string, data_cadastro timestamp, telefone string\").option(\"header\", \"false\").csv(\"dados/clientes/*.csv\")\n",
    "\n",
    "# filtrando o dataframe para excluir o arquivo clientes-001.csv\n",
    "clientes_sem_header = clientes.filter(~input_file_name().rlike(\"clients-001.csv\"))\n",
    "clientes_com_header = spark.read.option(\"sep\", \";\").schema(\"id int, nome string, email string, data_cadastro timestamp, telefone string\").option(\"header\", \"true\").csv(\"dados/clientes/clients-001.csv\")\n",
    "\n",
    "# unindo os dois dataframes\n",
    "clientes = clientes_com_header.union(clientes_sem_header)\n",
    "\n",
    "clientes.count()\n",
    "clientes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction-in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leitura de todos os arquivos CSV na pasta transaction-in\n",
    "transaction_in = spark.read.option(\"sep\", \";\").schema(\"id int, cliente_id int, valor double, data timestamp\").option(\"header\", \"false\").csv(\"/home/azureuser/transaction-in/*.csv\")\n",
    "\n",
    "# filtrando o dataframe para excluir o arquivo transaction-in-001.csv\n",
    "transaction_in_sem_header = transaction_in.filter(~input_file_name().rlike(\"transaction-in-001.csv\"))\n",
    "transaction_in_com_header = spark.read.option(\"sep\", \";\").schema(\"id int, cliente_id int, valor double, data timestamp\").option(\"header\", \"true\").csv(\"/home/azureuser/transaction-in/transaction-in-001.csv\")\n",
    "\n",
    "# unindo os dois dataframes\n",
    "transaction_in = transaction_in_com_header.union(transaction_in_sem_header)\n",
    "\n",
    "transaction_in.count()\n",
    "transaction_in.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction-out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leitura de todos os arquivos CSV na pasta transaction-out\n",
    "transaction_out = spark.read.option(\"sep\", \";\").schema(\"id int, cliente_id int, valor double, data timestamp\").option(\"header\", \"false\").csv(\"/home/azureuser/transaction-out/*.csv\")\n",
    "\n",
    "# filtrando o dataframe para excluir o arquivo transaction-out-001.csv\n",
    "transaction_out_sem_header = transaction_out.filter(~input_file_name().rlike(\"transaction-out-001.csv\"))\n",
    "transaction_out_com_header = spark.read.option(\"sep\", \";\")\\\n",
    "    .schema(\"id int, cliente_id int, valor double, data timestamp\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(\"/home/azureuser/transaction-out/transaction-out-001.csv\")\n",
    "\n",
    "# unindo os dois dataframes\n",
    "transaction_out = transaction_out_com_header.union(transaction_out_sem_header)\n",
    "\n",
    "transaction_out.count()\n",
    "transaction_out.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
