{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Sistema Bancario - Projeto Integrador Grupo 05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurando o ambiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/azureuser/.local/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /home/azureuser/.local/lib/python3.8/site-packages (from pyspark) (0.10.9.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
    "!tar -xvzf spark-3.3.2-bin-hadoop3.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/azureuser/spark-3.3.2-bin-hadoop3\"\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"Read CSV\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando os dados:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+-------------------+----------------+\n",
      "| id|                nome|               email|      data_cadastro|        telefone|\n",
      "+---+--------------------+--------------------+-------------------+----------------+\n",
      "|641|Priscila Felix do...|priscila-felix-do...|2021-03-28 18:46:57|+55(30)2227-2428|\n",
      "| 94|             idelmon|idelmon_94@gmail.com|2019-09-19 12:33:19|+55(29)3027-2026|\n",
      "|584|Liliane soares da...|liliane-soares-da...|2021-02-10 19:15:30|+55(21)2024-2520|\n",
      "|580|Fagner jose dos s...|fagner-jose-dos-s...|2021-02-07 01:47:04|+55(24)2624-2029|\n",
      "| 21|               Cildo|  cildo_21@gmail.com|2019-07-30 11:40:10|+55(21)2222-2422|\n",
      "|582|Nielton da Silva ...|nielton-da-silva-...|2021-02-09 00:11:22|+55(27)2028-2828|\n",
      "|586|Armando Teles da ...|armando-teles-da-...|2021-02-12 15:20:14|+55(27)2720-2230|\n",
      "|151|            Fabricio|fabricio_151@gmai...|2019-10-14 21:16:27|+55(20)2121-2326|\n",
      "| 83|       Flavio junior|flavio-junior_83@...|2019-09-11 15:24:00|+55(22)2028-2122|\n",
      "|587|              Wagner|wagner_587@gmail.com|2021-02-14 13:56:25|+55(28)2620-2421|\n",
      "|585|         Jeine maria|jeine-maria_585@g...|2021-02-11 23:49:04|+55(27)2528-2523|\n",
      "|534|                Thay|  thay_534@gmail.com|2020-12-19 02:10:58|+55(29)2327-2626|\n",
      "|639|Sindini vieira do...|sindini-vieira-do...|2021-03-21 16:07:04|+55(28)2126-2123|\n",
      "|528|           Aparecido|aparecido_528@gma...|2020-12-12 20:02:38|+55(26)2721-2828|\n",
      "|636|         Edy Navarro|edy-navarro_636@g...|2021-03-04 11:29:03|+55(30)3024-2821|\n",
      "|626|        Moab Tenorio|moab-tenorio_626@...|2021-02-21 14:49:26|+55(29)2921-2730|\n",
      "|627|     Jose Franciusci|jose-franciusci_6...|2021-02-21 14:53:39|+55(29)2222-2227|\n",
      "|637|Crislane luiz da ...|crislane-luiz-da-...|2021-03-07 03:11:09|+55(30)2821-2024|\n",
      "|635|             Emerson|emerson_635@gmail...|2021-03-03 23:03:31|+55(20)2224-2829|\n",
      "|638|               Italo| italo_638@gmail.com|2021-03-15 23:19:48|+55(25)2323-2424|\n",
      "+---+--------------------+--------------------+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# leitura de todos os arquivos CSV na pasta clientes\n",
    "clientes = spark.read.option(\"sep\", \";\").schema(\"id int, nome string, email string, data_cadastro timestamp, telefone string\").option(\"header\", \"false\").csv(\"dados/clientes/*.csv\")\n",
    "\n",
    "# filtrando o dataframe para excluir o arquivo clientes-001.csv\n",
    "clientes_sem_header = clientes.filter(~input_file_name().rlike(\"clients-001.csv\"))\n",
    "clientes_com_header = spark.read.option(\"sep\", \";\").schema(\"id int, nome string, email string, data_cadastro timestamp, telefone string\").option(\"header\", \"true\").csv(\"dados/clientes/clients-001.csv\")\n",
    "\n",
    "# unindo os dois dataframes\n",
    "clientes = clientes_com_header.union(clientes_sem_header)\n",
    "\n",
    "clientes.count()\n",
    "clientes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction-in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----+-------------------+\n",
      "|  id|cliente_id|valor|               data|\n",
      "+----+----------+-----+-------------------+\n",
      "|8615|       586|  0.2|2022-01-19 20:12:26|\n",
      "|8613|       586|  0.2|2022-01-19 20:11:25|\n",
      "|8611|       586|  0.2|2022-01-19 20:10:05|\n",
      "|8606|       910|300.0|2022-01-19 19:59:36|\n",
      "|8604|        76|100.0|2022-01-18 12:48:14|\n",
      "|8603|        76|100.0|2022-01-18 12:48:04|\n",
      "|8602|        76|100.0|2022-01-18 12:47:47|\n",
      "|8601|        76|100.0|2022-01-18 12:47:43|\n",
      "|8600|        76|100.0|2022-01-18 12:47:39|\n",
      "|8599|        76|100.0|2022-01-18 12:43:05|\n",
      "|8598|        76|100.0|2022-01-18 12:42:56|\n",
      "|8597|        76|100.0|2022-01-18 12:40:28|\n",
      "|8596|        76|100.0|2022-01-18 12:38:19|\n",
      "|8595|        76|100.0|2022-01-18 12:37:59|\n",
      "|8594|        76|100.0|2022-01-18 12:37:29|\n",
      "|8593|        76|100.0|2022-01-18 12:37:19|\n",
      "|8592|       907| 10.0|2022-01-18 12:30:26|\n",
      "|8591|       907| 10.0|2022-01-18 12:30:14|\n",
      "|8590|       907| 10.0|2022-01-18 12:30:10|\n",
      "|8589|       907| 10.0|2022-01-18 12:30:04|\n",
      "+----+----------+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# leitura de todos os arquivos CSV na pasta transaction-in\n",
    "transaction_in = spark.read.option(\"sep\", \";\").schema(\"id int, cliente_id int, valor double, data timestamp\").option(\"header\", \"false\").csv(\"/home/azureuser/transaction-in/*.csv\")\n",
    "\n",
    "# filtrando o dataframe para excluir o arquivo transaction-in-001.csv\n",
    "transaction_in_sem_header = transaction_in.filter(~input_file_name().rlike(\"transaction-in-001.csv\"))\n",
    "transaction_in_com_header = spark.read.option(\"sep\", \";\").schema(\"id int, cliente_id int, valor double, data timestamp\").option(\"header\", \"true\").csv(\"/home/azureuser/transaction-in/transaction-in-001.csv\")\n",
    "\n",
    "# unindo os dois dataframes\n",
    "transaction_in = transaction_in_com_header.union(transaction_in_sem_header)\n",
    "\n",
    "transaction_in.count()\n",
    "transaction_in.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction-out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+-------------------+\n",
      "|  id|cliente_id| valor|               data|\n",
      "+----+----------+------+-------------------+\n",
      "|8607|       910|  -2.0|2022-01-19 20:15:26|\n",
      "|8608|       910|  -2.0|2022-01-19 20:14:56|\n",
      "|8609|       910|  -2.0|2022-01-19 20:14:26|\n",
      "|8610|       910|  -2.0|2022-01-19 20:13:56|\n",
      "|8612|       910|  -2.0|2022-01-19 20:13:26|\n",
      "|8614|       910|  -2.0|2022-01-19 20:12:56|\n",
      "|8573|       671| -10.0|2022-01-13 15:21:25|\n",
      "|8574|       671| -10.0|2022-01-13 15:20:55|\n",
      "|8575|       671|  -5.0|2022-01-13 15:20:25|\n",
      "|8576|       671| -10.0|2022-01-13 15:19:55|\n",
      "|8577|       671|  -2.0|2022-01-13 15:19:25|\n",
      "|8580|       671| -10.0|2022-01-13 15:18:55|\n",
      "|8581|       671|-100.0|2022-01-13 15:18:25|\n",
      "|8582|       671| -15.0|2022-01-13 15:17:55|\n",
      "|8583|       671|-100.0|2022-01-13 15:17:25|\n",
      "|8584|       671|  -2.0|2022-01-13 15:16:55|\n",
      "|8579|       370|  -7.0|2022-01-13 03:44:57|\n",
      "|8578|       370|  -3.0|2022-01-13 03:37:43|\n",
      "|8572|       671|  -2.0|2022-01-12 19:43:20|\n",
      "|8539|       671| -10.0|2022-01-12 17:16:29|\n",
      "+----+----------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# leitura de todos os arquivos CSV na pasta transaction-out\n",
    "transaction_out = spark.read.option(\"sep\", \";\").schema(\"id int, cliente_id int, valor double, data timestamp\").option(\"header\", \"false\").csv(\"/home/azureuser/transaction-out/*.csv\")\n",
    "\n",
    "# filtrando o dataframe para excluir o arquivo transaction-out-001.csv\n",
    "transaction_out_sem_header = transaction_out.filter(~input_file_name().rlike(\"transaction-out-001.csv\"))\n",
    "transaction_out_com_header = spark.read.option(\"sep\", \";\")\\\n",
    "    .schema(\"id int, cliente_id int, valor double, data timestamp\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(\"/home/azureuser/transaction-out/transaction-out-001.csv\")\n",
    "\n",
    "# unindo os dois dataframes\n",
    "transaction_out = transaction_out_com_header.union(transaction_out_sem_header)\n",
    "\n",
    "transaction_out.count()\n",
    "transaction_out.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criando o dataframe dos union das duas tabelas de transações (in, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem de todas as transações: 7272\n",
      "+----+----------+-----+-------------------+\n",
      "|id  |cliente_id|valor|data               |\n",
      "+----+----------+-----+-------------------+\n",
      "|8615|586       |0.2  |2022-01-19 20:12:26|\n",
      "|8613|586       |0.2  |2022-01-19 20:11:25|\n",
      "|8611|586       |0.2  |2022-01-19 20:10:05|\n",
      "|8606|910       |300.0|2022-01-19 19:59:36|\n",
      "|8604|76        |100.0|2022-01-18 12:48:14|\n",
      "|8603|76        |100.0|2022-01-18 12:48:04|\n",
      "|8602|76        |100.0|2022-01-18 12:47:47|\n",
      "|8601|76        |100.0|2022-01-18 12:47:43|\n",
      "|8600|76        |100.0|2022-01-18 12:47:39|\n",
      "|8599|76        |100.0|2022-01-18 12:43:05|\n",
      "|8598|76        |100.0|2022-01-18 12:42:56|\n",
      "|8597|76        |100.0|2022-01-18 12:40:28|\n",
      "|8596|76        |100.0|2022-01-18 12:38:19|\n",
      "|8595|76        |100.0|2022-01-18 12:37:59|\n",
      "|8594|76        |100.0|2022-01-18 12:37:29|\n",
      "|8593|76        |100.0|2022-01-18 12:37:19|\n",
      "|8592|907       |10.0 |2022-01-18 12:30:26|\n",
      "|8591|907       |10.0 |2022-01-18 12:30:14|\n",
      "|8590|907       |10.0 |2022-01-18 12:30:10|\n",
      "|8589|907       |10.0 |2022-01-18 12:30:04|\n",
      "+----+----------+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_transactions ----- O Dataframe de todas as transações (in e out)\n",
    "df_transactions = (transaction_in.union(transaction_out))\n",
    "\n",
    "print(f\"Contagem de todas as transações: {df_transactions.count()}\")\n",
    "\n",
    "df_transactions.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adicionar na tabela df_transaction uma coluna para a diferença de tempo entre as transações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-------------------+------------------+\n",
      "| id|cliente_id|valor|               data|diferenca_de_tempo|\n",
      "+---+----------+-----+-------------------+------------------+\n",
      "| 58|         2|-20.0|2019-09-23 22:49:44|                 0|\n",
      "| 60|         2| -5.0|2019-09-28 02:12:19|            357755|\n",
      "| 99|         2| -2.0|2019-10-02 18:21:43|            403764|\n",
      "|100|         2| -2.0|2019-10-02 18:23:58|               135|\n",
      "|101|         2|-10.0|2019-10-02 19:37:18|              4400|\n",
      "|220|         2| -2.0|2019-11-27 22:38:20|           4849262|\n",
      "|226|         2| -2.0|2019-11-28 19:58:05|             76785|\n",
      "|227|         2| -2.0|2019-11-28 19:58:24|                19|\n",
      "|228|         2| -2.0|2019-11-28 19:58:44|                20|\n",
      "|231|         2| -2.0|2019-11-30 14:38:14|            153570|\n",
      "|232|         2| -2.0|2019-11-30 14:38:49|                35|\n",
      "|233|         2| -2.0|2019-11-30 14:41:20|               151|\n",
      "|234|         2| -2.0|2019-11-30 14:45:02|               222|\n",
      "|244|         2| -2.0|2019-12-03 19:48:31|            277409|\n",
      "|245|         2| -2.0|2019-12-03 19:48:51|                20|\n",
      "|246|         2| -2.0|2019-12-03 19:49:08|                17|\n",
      "|247|         2| -2.0|2019-12-03 19:49:40|                32|\n",
      "|249|         2| -2.0|2019-12-04 01:19:58|             19818|\n",
      "|250|         2| -2.0|2019-12-04 01:21:54|               116|\n",
      "|251|         2| -2.0|2019-12-04 01:23:02|                68|\n",
      "+---+----------+-----+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag, datediff, col, when, lit,desc, asc\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Essa window irá particionar a tabela através do cliente_id e ordenar essa partição por data\n",
    "window_spec = Window.partitionBy(\"cliente_id\").orderBy(\"data\")\n",
    "\n",
    "# Essa parte criar a coluna \"diferenca_de_tempo\" para o tempo entre as transações (obs.: O tempo será 0, quando a diferenca de tempo for nula, ou seja, quando subsequente houver um outro cliente e não houver diferença de tempo entre eles.\n",
    "diferenca_de_tempo = (col(\"data\").cast(\"long\") - \n",
    "                   lag(col(\"data\")).over(window_spec).cast(\"long\"))\n",
    "\n",
    "df_transactions = df_transactions.withColumn(\"diferenca_de_tempo\", when(diferenca_de_tempo.isNull(), lit(0)).otherwise(diferenca_de_tempo))\n",
    "\n",
    "df_transactions.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
